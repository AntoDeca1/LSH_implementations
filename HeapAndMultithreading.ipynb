{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here two utility functions\n",
    "1) hamming : Compute the hamming distance between two binary vectors\n",
    "2) process_block: Is the function executed by a thread in our ThreadPool and is responsible of updating the SharedHeap for the corresponding query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def hamming_distance(a, b):\n",
    "    return np.count_nonzero(a != b)\n",
    "\n",
    "\n",
    "def process_block(queries, database_block, shared_heap, start_index):\n",
    "    for i, query in enumerate(queries):\n",
    "        for j, db_vector in enumerate(database_block):\n",
    "            distance = hamming_distance(query, db_vector)\n",
    "            shared_heap.update(i, distance, start_index + j)\n",
    "    return \"Block processed\"  # Return a value to indicate completion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we present the implementation of a SharedHeapDataStracture useful for implementing a Faiss-Like LSH approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Heap is a datastructure based on binary three that could be easily implemented by using arrays. In particular we realize a Max-Heap.\n",
    "Max-Heap means that in the root of our binary three we will have the highest value with the property that each node must have descendents with a lower value\n",
    "This datastructure is modified concurrently by different thread(the reason of using Locks) and at the end will contain the top k(approximate) closest neighbour."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "class SharedMaxHeap:\n",
    "    def __init__(self, n_queries, k):\n",
    "        self.n_queries = n_queries\n",
    "        self.k = k\n",
    "        self.distances = np.full((n_queries, k), np.inf)\n",
    "        self.indices = np.full((n_queries, k), -1, dtype=np.int64)\n",
    "        self.locks = [threading.Lock() for _ in range(n_queries)]\n",
    "\n",
    "    def update(self, query_idx, distance, index):\n",
    "        with self.locks[query_idx]:\n",
    "            if distance < self.distances[query_idx, 0]:\n",
    "                self.distances[query_idx, 0] = distance\n",
    "                self.indices[query_idx, 0] = index\n",
    "                # Maintain heap property\n",
    "                i = 0\n",
    "                while 2 * i + 1 < self.k:  # Ha almeno un figlio\n",
    "                    left = 2 * i + 1\n",
    "                    right = 2 * i + 2\n",
    "                    largest = i\n",
    "                    if left < self.k and self.distances[query_idx, left] > self.distances[query_idx, largest]:\n",
    "                        largest = left\n",
    "                    if right < self.k and self.distances[query_idx, right] > self.distances[query_idx, largest]:\n",
    "                        largest = right\n",
    "                    if largest == i:  # Se invece il maggiore Ã¨ quello in cima stiamo bene\n",
    "                        break\n",
    "                    self.distances[query_idx, i], self.distances[query_idx, largest] = \\\n",
    "                        self.distances[query_idx, largest], self.distances[query_idx, i]\n",
    "                    self.indices[query_idx, i], self.indices[query_idx, largest] = \\\n",
    "                        self.indices[query_idx, largest], self.indices[query_idx, i]\n",
    "                    i = largest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def knn_hamming_search(queries, database, k, block_size=12):\n",
    "    n_queries, n_database = len(queries), len(database)\n",
    "    shared_heap = SharedMaxHeap(n_queries, k)\n",
    "\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor() as executor:  # Default n_workers=Logical CPU Cores+ 4\n",
    "        for i in range(0, n_database, block_size):  # For each block\n",
    "            database_block = database[i:min(i + block_size, n_database)]  # Pick a block from the index\n",
    "            future = executor.submit(process_block, queries, database_block, shared_heap, i)  # Submit a\n",
    "            futures.append(future)\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in as_completed(futures):\n",
    "            future.result()  # This will raise an exception if the task failed\n",
    "\n",
    "    # Sort results for each query\n",
    "    results = []\n",
    "    for i in range(n_queries):\n",
    "        indices = shared_heap.indices[i]\n",
    "        distances = shared_heap.distances[i]\n",
    "        sorted_pairs = sorted(zip(distances, indices))\n",
    "        sorted_distances, sorted_indices = zip(*sorted_pairs)\n",
    "        results.append((list(sorted_indices), list(sorted_distances)))\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07957983016967773\n",
      "Top 5 matches for the first query:\n",
      "Global Indices: [0, 77, 34, 74, 84]\n",
      "Distances: [0.0, 225.0, 236.0, 237.0, 238.0]\n",
      "Search completed.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    queries = np.random.randint(0, 2, (100, 512))  # 100 query vectors of 64 bits each\n",
    "    database = np.random.randint(0, 2, (20000, 512))  # 10000 database vectors of 64 bits each\n",
    "    k = 5  # find top 5 nearest neighbors\n",
    "\n",
    "    prima = time.time()\n",
    "    results = knn_hamming_search(queries, queries, k)\n",
    "    print(time.time() - prima)\n",
    "\n",
    "    # Print results for the first query\n",
    "    print(\"Top 5 matches for the first query:\")\n",
    "    print(\"Global Indices:\", results[0][0])\n",
    "    print(\"Distances:\", results[0][1])\n",
    "\n",
    "    print(\"Search completed.\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
